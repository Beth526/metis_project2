{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scraping Discogs.com with Beautiful Soup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import random\n",
    "import pandas as pd\n",
    "import re\n",
    "import time\n",
    "import numpy as np\n",
    "import pickle\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set the user agent to appear as though a browser is accessing the page"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_agent = {'User-agent': 'Mozilla/5.0'}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scraping the first few pages (search results of the top collected records) for over 1000 records and some data and links to pages that contain more data on each."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set up the lists outside of the scraping loop, so if it fails some data is still saved"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "artists=[]\n",
    "years=[]\n",
    "release_countries=[]\n",
    "labels=[]\n",
    "artist_links=[]\n",
    "albums=[]\n",
    "album_links=[]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The loop below collects info from the first 20 pages of search results by appending numbers to the search result address"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(1,21):\n",
    "    '''collects most collected records data from discogs search pages'''\n",
    "    #set the url based on the search page number\n",
    "    if i==1:\n",
    "        url = \"https://www.discogs.com/search/?sort=have%2Cdesc&ev=em_rs&format_exact=Vinyl&layout=sm\"\n",
    "    if i > 0:\n",
    "        url = \"https://www.discogs.com/search/?sort=have%2Cdesc&ev=em_rs&format_exact=Vinyl&layout=sm&page=\" + str(i)\n",
    "    \n",
    "    #wait to not get banned from discogs \n",
    "    time.sleep(10+2*random.random())\n",
    "    \n",
    "    #use response to get the page html\n",
    "    response = requests.get(url, headers = user_agent)\n",
    "    page = response.text\n",
    "    \n",
    "    #use beautiful soup to create a soup object of html to parse\n",
    "    soup = BeautifulSoup(page, \"lxml\")\n",
    "    \n",
    "    #adding artists, artist links, albums, main album links, label, year, and country to the lists\n",
    "    #if there are multiple artists or labels it takes only the first\n",
    "    #try and except is used in case an element can't be found, numpy NaN is appended instead\n",
    "    for element in soup.find(id='search_results').find_all(class_=\"card_body\"):\n",
    "        try:\n",
    "            years.append(element.find_all(class_=\"card_release_year\")[0].text)\n",
    "        except:\n",
    "            years.append(np.nan)\n",
    "        try:\n",
    "            labels.append(element.find('p', class_=\"card_info\").find_all('a')[0].text)\n",
    "        except:\n",
    "            labels.append(np.nan)\n",
    "        try:\n",
    "            release_countries.append(element.find_all(class_=\"card_release_country\")[0].text)\n",
    "        except:\n",
    "            release_countries.append(np.nan)\n",
    "        try:\n",
    "            albums.append(element.find_all(\"a\", class_=\"search_result_title\")[0].text)\n",
    "        except:\n",
    "            albums.append(np.nan)\n",
    "        try:\n",
    "            album_links.append('https://www.discogs.com'+element.find_all('a', class_=\"search_result_title\")[0].get('href'))\n",
    "        except:\n",
    "            album_links.append(np.nan)\n",
    "        try:\n",
    "            artists.append(element.find_all('a')[0].text)\n",
    "        except:\n",
    "            artists.append(np.nan)\n",
    "        try:\n",
    "            artist_links.append('https://www.discogs.com'+element.find_all('a')[0].get('href'))\n",
    "        except:\n",
    "            artist_links.append(np.nan)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that when I originally did this, I had actually scraped the first page of search results twice, by starting at 0 rather than one. I corrected the mistake much later after noticing duplicates in my final data frame by using the below code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#top_1000_collected_final_df = top_1050_collected_final_df.drop(np.arange(50,100), axis=0)\n",
    "#top_1000_collected_final_df.reset_index(inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check the length of all lists is equal and put into a pandas dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_all_collected = pd.DataFrame({'artists':artists,'albums':albums, 'artist_links':artist_links, 'album_links':album_links, 'release_year':years, 'label':labels, 'release_country':release_countries})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>artists</th>\n",
       "      <th>albums</th>\n",
       "      <th>artist_links</th>\n",
       "      <th>album_links</th>\n",
       "      <th>release_year</th>\n",
       "      <th>label</th>\n",
       "      <th>release_country</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1045</th>\n",
       "      <td>The Beatles</td>\n",
       "      <td>The Beatles At The Hollywood Bowl</td>\n",
       "      <td>https://www.discogs.com/artist/82730-The-Beatles</td>\n",
       "      <td>https://www.discogs.com/The-Beatles-The-Beatle...</td>\n",
       "      <td>1977</td>\n",
       "      <td>Parlophone</td>\n",
       "      <td>UK</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1046</th>\n",
       "      <td>Fine Young Cannibals</td>\n",
       "      <td>The Raw &amp; The Cooked</td>\n",
       "      <td>https://www.discogs.com/artist/4711-Fine-Young...</td>\n",
       "      <td>https://www.discogs.com/Fine-Young-Cannibals-T...</td>\n",
       "      <td>1988</td>\n",
       "      <td>London Records</td>\n",
       "      <td>UK</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1047</th>\n",
       "      <td>Elton John</td>\n",
       "      <td>A Single Man</td>\n",
       "      <td>https://www.discogs.com/artist/57103-Elton-John</td>\n",
       "      <td>https://www.discogs.com/Elton-John-A-Single-Ma...</td>\n",
       "      <td>1978</td>\n",
       "      <td>The Rocket Record Company</td>\n",
       "      <td>UK</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1048</th>\n",
       "      <td>David Bowie</td>\n",
       "      <td>Never Let Me Down</td>\n",
       "      <td>https://www.discogs.com/artist/10263-David-Bowie</td>\n",
       "      <td>https://www.discogs.com/David-Bowie-Never-Let-...</td>\n",
       "      <td>1987</td>\n",
       "      <td>EMI America</td>\n",
       "      <td>UK</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1049</th>\n",
       "      <td>Daryl Hall + John Oates</td>\n",
       "      <td>H2O</td>\n",
       "      <td>https://www.discogs.com/artist/95886-Daryl-Hal...</td>\n",
       "      <td>https://www.discogs.com/Daryl-Hall-John-Oates-...</td>\n",
       "      <td>1982</td>\n",
       "      <td>RCA</td>\n",
       "      <td>US</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                      artists                             albums  \\\n",
       "1045              The Beatles  The Beatles At The Hollywood Bowl   \n",
       "1046     Fine Young Cannibals               The Raw & The Cooked   \n",
       "1047               Elton John                       A Single Man   \n",
       "1048              David Bowie                  Never Let Me Down   \n",
       "1049  Daryl Hall + John Oates                                H2O   \n",
       "\n",
       "                                           artist_links  \\\n",
       "1045   https://www.discogs.com/artist/82730-The-Beatles   \n",
       "1046  https://www.discogs.com/artist/4711-Fine-Young...   \n",
       "1047    https://www.discogs.com/artist/57103-Elton-John   \n",
       "1048   https://www.discogs.com/artist/10263-David-Bowie   \n",
       "1049  https://www.discogs.com/artist/95886-Daryl-Hal...   \n",
       "\n",
       "                                            album_links release_year  \\\n",
       "1045  https://www.discogs.com/The-Beatles-The-Beatle...         1977   \n",
       "1046  https://www.discogs.com/Fine-Young-Cannibals-T...         1988   \n",
       "1047  https://www.discogs.com/Elton-John-A-Single-Ma...         1978   \n",
       "1048  https://www.discogs.com/David-Bowie-Never-Let-...         1987   \n",
       "1049  https://www.discogs.com/Daryl-Hall-John-Oates-...         1982   \n",
       "\n",
       "                          label release_country  \n",
       "1045                 Parlophone              UK  \n",
       "1046             London Records              UK  \n",
       "1047  The Rocket Record Company              UK  \n",
       "1048                EMI America              UK  \n",
       "1049                        RCA              US  "
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "top_all_collected.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1050 entries, 0 to 1049\n",
      "Data columns (total 7 columns):\n",
      " #   Column           Non-Null Count  Dtype \n",
      "---  ------           --------------  ----- \n",
      " 0   artists          1050 non-null   object\n",
      " 1   albums           1050 non-null   object\n",
      " 2   artist_links     1050 non-null   object\n",
      " 3   album_links      1050 non-null   object\n",
      " 4   release_year     1047 non-null   object\n",
      " 5   label            1050 non-null   object\n",
      " 6   release_country  1049 non-null   object\n",
      "dtypes: object(7)\n",
      "memory usage: 57.5+ KB\n"
     ]
    }
   ],
   "source": [
    "top_all_collected.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('top_1050_collected_records.pickle', 'wb') as save_file:\n",
    "    pickle.dump(top_all_collected, save_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scraping the main album pages for song info, and links to the first album version"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since albums come in many versions (or pressings), the price varies a lot based on the pressing number. Original pressings are worth the most usually. Since most popular albums were pressed around 100 times or more, I'd have to access that many pages to get the price for each pressing. I decided to focus on predicting the price of the first pressing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "first_release_links = []\n",
    "number_of_songs= []\n",
    "average_song_length= []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The loop below scrapes the main album pages, gets the link to the first realse of that album, and also counts the number of songs, and calculates the average run time of a song on that album. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "for url in top_all_collected['album_links']:\n",
    "    #from album main page\n",
    "    time.sleep(10+2*random.random())\n",
    "    response = requests.get(url, headers = user_agent)\n",
    "    page = response.text\n",
    "    soup = BeautifulSoup(page, \"lxml\")\n",
    "    \n",
    "    #link to album 1st release page\n",
    "    try:\n",
    "        first_release_links.append('https://www.discogs.com' + soup.find('table', id='versions').find('td', class_='title').find_all('a')[0].get('href'))\n",
    "    except:\n",
    "        first_release_links.append(np.nan)\n",
    "    \n",
    "    #number of songs\n",
    "    s=0\n",
    "    try:\n",
    "        for element in soup.find('table', class_=\"playlist\").find_all('tr', class_=\"tracklist_track track\"):\n",
    "            for song in element.find('span', class_=\"tracklist_track_title\"):\n",
    "                s += 1\n",
    "        number_of_songs.append(s)\n",
    "    except:\n",
    "        number_of_songs.append(np.nan)\n",
    "    \n",
    "    #average song length\n",
    "    form='%M:%S'\n",
    "    song_lengths=[]\n",
    "    try:\n",
    "        for element in soup.find('table', class_=\"playlist\").find_all('tr', class_=\"tracklist_track track\"):\n",
    "            t = element.find('td', class_=\"tracklist_track_duration\").find('span').text\n",
    "            t = datetime.strptime(t,form)\n",
    "            song_lengths.append(t)\n",
    "        avg_time=datetime.strftime(datetime.fromtimestamp(sum(map(datetime.timestamp,song_lengths))/len(song_lengths)),\"%M:%S\")\n",
    "        average_song_length.append(avg_time)\n",
    "    except:\n",
    "        average_song_length.append(np.nan)\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "album_main_pages = pd.DataFrame({'first_release_links':first_release_links,'number_of_songs':number_of_songs,'average_song_length':average_song_length})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I noticed that the number of songs was one lower than expected for each album. I am not sure why based on the code above, but will look into this. It's possible average song length is also missing one song. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "album_main_pages['number_of_songs']=album_main_pages['number_of_songs']+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1050 entries, 0 to 1049\n",
      "Data columns (total 3 columns):\n",
      " #   Column               Non-Null Count  Dtype \n",
      "---  ------               --------------  ----- \n",
      " 0   first_release_links  1050 non-null   object\n",
      " 1   number_of_songs      1050 non-null   int64 \n",
      " 2   average_song_length  831 non-null    object\n",
      "dtypes: int64(1), object(2)\n",
      "memory usage: 24.7+ KB\n"
     ]
    }
   ],
   "source": [
    "album_main_pages.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('album_main_pages_2.pickle', 'wb') as save_file:\n",
    "    pickle.dump(album_main_pages, save_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scraping the artist pages for data on number of albums, and years of albums"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This uses the artist links from the top_1050_collected_records table. I thought that sorting the artist's albums in reverse to get the last year of a release would be a good idea, but a lot of the artists have compilation albums released years after the band broke up. Surprisingly, the year the band broke up is not readily available."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_artist_albums = []\n",
    "artist_first_years = []\n",
    "artist_last_years = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "for url in top_1050_collected_records['artist_links']:\n",
    "    #get info from artist pages\n",
    "    response = requests.get(url, headers = user_agent)\n",
    "    time.sleep(5+1*random.random())\n",
    "    page = response.text\n",
    "    soup = BeautifulSoup(page, \"lxml\")\n",
    "    \n",
    "    #total albums\n",
    "    try:\n",
    "        total_artist_albums.append(soup.find(id=\"discography_wrapper\").find_all('span', class_='facet_count')[1].text)\n",
    "    except:\n",
    "        total_artist_albums.append(np.nan)\n",
    "        \n",
    "    #first album year\n",
    "    try:\n",
    "        artist_first_years.append(soup.find('table', id=\"artist\").find_all('td', class_=\"year has_header\")[0].text)\n",
    "    except:\n",
    "        artist_first_years.append(np.nan)\n",
    "        \n",
    "    #last album year\n",
    "    try:\n",
    "        url=url + \"?sort=year%2Cdesc&limit=25\"\n",
    "        time.sleep(5+1*random.random())\n",
    "        response = requests.get(url, headers = user_agent)\n",
    "        page = response.text\n",
    "        soup = BeautifulSoup(page, \"lxml\")\n",
    "        artist_last_years.append(soup.find('table', id=\"artist\").find_all('td', class_=\"year has_header\")[0].text)\n",
    "    except:\n",
    "        artist_last_years.append(np.nan)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "artist_data = pd.DataFrame({'total_artist_albums':total_artist_albums,'artist_last_years':artist_last_years,'artist_first_years':artist_first_years})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('artist_data.pickle', 'wb') as write_file:\n",
    "    pickle.dump(artist_data, write_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scraping the first release page for each album for many stats and the target variable of median price"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "users_have = []\n",
    "users_want = []\n",
    "user_rating = []\n",
    "lowest_price = []\n",
    "median_price = []\n",
    "highest_price = []\n",
    "last_sold = []\n",
    "number_for_sale = []\n",
    "styles = []\n",
    "genres = []\n",
    "versions = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "for url in album_main_pages['first_release_links'][46:]:\n",
    "    #get info from the first release of an album page\n",
    "    response = requests.get(url, headers = user_agent)\n",
    "    time.sleep(5+1*random.random())\n",
    "    page = response.text\n",
    "    soup = BeautifulSoup(page, \"lxml\")\n",
    "    \n",
    "    #getting info for the record stats, including price\n",
    "    try:\n",
    "        stats =[element for element in soup.find(id = \"statistics\").find_all('li')]\n",
    "        \n",
    "        users_have.append(re.search(':\\n([0-9]*)',stats[0].text).group(1))\n",
    "        users_want.append(re.search(':\\n([0-9]*)',stats[1].text).group(1))\n",
    "        user_rating.append(re.search(':\\n([0-9].[0-9]{1,2})', stats[2].text).group(1))\n",
    "        lowest_price.append(re.search('\\$([0-9,]*.[0-9]{2})',stats[5].text).group(1))\n",
    "        median_price.append(re.search('\\$([0-9,]*.[0-9]{2})',stats[6].text).group(1))\n",
    "        highest_price.append(re.search('\\$([0-9,]*.[0-9]{2})',stats[7].text).group(1))\n",
    "        last_sold.append(re.search(':\\n([0-9]{2} [A-z]{3} [0-9]{2})',stats[4].text).group(1))\n",
    "        \n",
    "    except:\n",
    "        users_have.append(np.nan)\n",
    "        users_want.append(np.nan)\n",
    "        user_rating.append(np.nan)\n",
    "        lowest_price.append(np.nan)\n",
    "        median_price.append(np.nan)\n",
    "        highest_price.append(np.nan)\n",
    "        last_sold.append(np.nan)\n",
    "\n",
    "    #the number of versions\n",
    "    try:\n",
    "        versions.append(re.search('of ([0-9]*)\\)',soup.find('h3',{'data-for':'.m_versions'}).text).group(1))\n",
    "    except:\n",
    "        versions.append(np.nan)\n",
    "        \n",
    "    #getting for sale info from marketplace section\n",
    "    try:\n",
    "        element = soup.find(class_ = \"marketplace_for_sale_count\").find('strong').text\n",
    "        number_for_sale.append(re.search('([0-9,]*)',element).group())\n",
    "    except:\n",
    "        number_for_sale.append(np.nan)\n",
    "        \n",
    "    #find genre and styles (may be multiple, so a list)\n",
    "    try:\n",
    "        all_styles=[]\n",
    "        for link in soup.find(class_=\"profile\").find_all(\"a\"):\n",
    "            if 'style' in link.get('href'):\n",
    "                all_styles.append(link.text)\n",
    "        styles.append(all_styles)\n",
    "    except:\n",
    "        styles.append(np.nan)\n",
    "        \n",
    "    try:\n",
    "        all_genres=[]\n",
    "        for link in soup.find(class_=\"profile\").find_all(\"a\"):\n",
    "            if 'genre' in link.get('href'):\n",
    "                all_genres.append(link.text)\n",
    "        genres.append(all_genres)\n",
    "    except:\n",
    "        genres.append(np.nan)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "first_version_page_df = pd.DataFrame({'users_have':users_have,'users_want':users_want,'user_rating':user_rating,'lowest_price':lowest_price,'median_price':median_price,'highest_price':highest_price,'last_sold':last_sold,'number_for_sale':number_for_sale,'styles':styles,'genres':genres,'versions':versions})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('first_edition_album_info.pickle', 'wb') as write_file:\n",
    "    pickle.dump(first_edition_album_info, write_file)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1050 entries, 0 to 1049\n",
      "Data columns (total 12 columns):\n",
      " #   Column           Non-Null Count  Dtype \n",
      "---  ------           --------------  ----- \n",
      " 0   index            1050 non-null   int64 \n",
      " 1   users_have       1032 non-null   object\n",
      " 2   users_want       1032 non-null   object\n",
      " 3   user_rating      1030 non-null   object\n",
      " 4   lowest_price     1027 non-null   object\n",
      " 5   median_price     1027 non-null   object\n",
      " 6   highest_price    1027 non-null   object\n",
      " 7   last_sold        1027 non-null   object\n",
      " 8   styles           1049 non-null   object\n",
      " 9   genres           1049 non-null   object\n",
      " 10  number_for_sale  1034 non-null   object\n",
      " 11  versions         1033 non-null   object\n",
      "dtypes: int64(1), object(11)\n",
      "memory usage: 98.6+ KB\n"
     ]
    }
   ],
   "source": [
    "first_edition_album_info.info()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
