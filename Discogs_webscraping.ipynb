{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scraping Discogs.com with Beautiful Soup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import random\n",
    "import pandas as pd\n",
    "import re\n",
    "import time\n",
    "import numpy as np\n",
    "import pickle\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set the user agent to appear as though a browser is accessing the page"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_agent = {'User-agent': 'Mozilla/5.0'}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scraping the first few pages (search results of the top collected records) for over 1000 records and some data and links to pages that contain more data on each."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set up the lists outside of the scraping loop, so if it fails some data is still saved"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "artists=[]\n",
    "years=[]\n",
    "release_countries=[]\n",
    "labels=[]\n",
    "artist_links=[]\n",
    "albums=[]\n",
    "album_links=[]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The loop below collects info from the first 20 pages of search results by appending numbers to the search result address"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(1,21):\n",
    "    '''collects most collected records data from discogs search pages'''\n",
    "    #set the url based on the search page number\n",
    "    if i==1:\n",
    "        url = \"https://www.discogs.com/search/?sort=have%2Cdesc&ev=em_rs&format_exact=Vinyl&layout=sm\"\n",
    "    if i > 0:\n",
    "        url = \"https://www.discogs.com/search/?sort=have%2Cdesc&ev=em_rs&format_exact=Vinyl&layout=sm&page=\" + str(i)\n",
    "    \n",
    "    #wait to not get banned from discogs \n",
    "    time.sleep(10+2*random.random())\n",
    "    \n",
    "    #use response to get the page html\n",
    "    response = requests.get(url, headers = user_agent)\n",
    "    page = response.text\n",
    "    \n",
    "    #use beautiful soup to create a soup object of html to parse\n",
    "    soup = BeautifulSoup(page, \"lxml\")\n",
    "    \n",
    "    #adding artists, artist links, albums, main album links, label, year, and country to the lists\n",
    "    #if there are multiple artists or labels it takes only the first\n",
    "    #try and except is used in case an element can't be found, numpy NaN is appended instead\n",
    "    for element in soup.find(id='search_results').find_all(class_=\"card_body\"):\n",
    "        try:\n",
    "            years.append(element.find_all(class_=\"card_release_year\")[0].text)\n",
    "        except:\n",
    "            years.append(np.nan)\n",
    "        try:\n",
    "            labels.append(element.find('p', class_=\"card_info\").find_all('a')[0].text)\n",
    "        except:\n",
    "            labels.append(np.nan)\n",
    "        try:\n",
    "            release_countries.append(element.find_all(class_=\"card_release_country\")[0].text)\n",
    "        except:\n",
    "            release_countries.append(np.nan)\n",
    "        try:\n",
    "            albums.append(element.find_all(\"a\", class_=\"search_result_title\")[0].text)\n",
    "        except:\n",
    "            albums.append(np.nan)\n",
    "        try:\n",
    "            album_links.append('https://www.discogs.com'+element.find_all('a', class_=\"search_result_title\")[0].get('href'))\n",
    "        except:\n",
    "            album_links.append(np.nan)\n",
    "        try:\n",
    "            artists.append(element.find_all('a')[0].text)\n",
    "        except:\n",
    "            artists.append(np.nan)\n",
    "        try:\n",
    "            artist_links.append('https://www.discogs.com'+element.find_all('a')[0].get('href'))\n",
    "        except:\n",
    "            artist_links.append(np.nan)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that when I originally did this, I had actually scraped the first page of search results twice, by starting at 0 rather than one. I corrected the mistake much later after noticing duplicates in my final data frame by using the below code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#top_1000_collected_final_df = top_1050_collected_final_df.drop(np.arange(50,100), axis=0)\n",
    "#top_1000_collected_final_df.reset_index(inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check the length of all lists is equal and put into a pandas dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(years)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_all_collected = pd.DataFrame({'artists':artists,'albums':albums, 'artist_links':artist_links, 'album_links':album_links, 'release_year':years, 'label':labels, 'release_country':release_countries})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>artists</th>\n",
       "      <th>albums</th>\n",
       "      <th>artist_links</th>\n",
       "      <th>album_links</th>\n",
       "      <th>release_year</th>\n",
       "      <th>label</th>\n",
       "      <th>release_country</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1045</th>\n",
       "      <td>The Beatles</td>\n",
       "      <td>The Beatles At The Hollywood Bowl</td>\n",
       "      <td>https://www.discogs.com/artist/82730-The-Beatles</td>\n",
       "      <td>https://www.discogs.com/The-Beatles-The-Beatle...</td>\n",
       "      <td>1977</td>\n",
       "      <td>Parlophone</td>\n",
       "      <td>UK</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1046</th>\n",
       "      <td>Fine Young Cannibals</td>\n",
       "      <td>The Raw &amp; The Cooked</td>\n",
       "      <td>https://www.discogs.com/artist/4711-Fine-Young...</td>\n",
       "      <td>https://www.discogs.com/Fine-Young-Cannibals-T...</td>\n",
       "      <td>1988</td>\n",
       "      <td>London Records</td>\n",
       "      <td>UK</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1047</th>\n",
       "      <td>Elton John</td>\n",
       "      <td>A Single Man</td>\n",
       "      <td>https://www.discogs.com/artist/57103-Elton-John</td>\n",
       "      <td>https://www.discogs.com/Elton-John-A-Single-Ma...</td>\n",
       "      <td>1978</td>\n",
       "      <td>The Rocket Record Company</td>\n",
       "      <td>UK</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1048</th>\n",
       "      <td>David Bowie</td>\n",
       "      <td>Never Let Me Down</td>\n",
       "      <td>https://www.discogs.com/artist/10263-David-Bowie</td>\n",
       "      <td>https://www.discogs.com/David-Bowie-Never-Let-...</td>\n",
       "      <td>1987</td>\n",
       "      <td>EMI America</td>\n",
       "      <td>UK</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1049</th>\n",
       "      <td>Daryl Hall + John Oates</td>\n",
       "      <td>H2O</td>\n",
       "      <td>https://www.discogs.com/artist/95886-Daryl-Hal...</td>\n",
       "      <td>https://www.discogs.com/Daryl-Hall-John-Oates-...</td>\n",
       "      <td>1982</td>\n",
       "      <td>RCA</td>\n",
       "      <td>US</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                      artists                             albums  \\\n",
       "1045              The Beatles  The Beatles At The Hollywood Bowl   \n",
       "1046     Fine Young Cannibals               The Raw & The Cooked   \n",
       "1047               Elton John                       A Single Man   \n",
       "1048              David Bowie                  Never Let Me Down   \n",
       "1049  Daryl Hall + John Oates                                H2O   \n",
       "\n",
       "                                           artist_links  \\\n",
       "1045   https://www.discogs.com/artist/82730-The-Beatles   \n",
       "1046  https://www.discogs.com/artist/4711-Fine-Young...   \n",
       "1047    https://www.discogs.com/artist/57103-Elton-John   \n",
       "1048   https://www.discogs.com/artist/10263-David-Bowie   \n",
       "1049  https://www.discogs.com/artist/95886-Daryl-Hal...   \n",
       "\n",
       "                                            album_links release_year  \\\n",
       "1045  https://www.discogs.com/The-Beatles-The-Beatle...         1977   \n",
       "1046  https://www.discogs.com/Fine-Young-Cannibals-T...         1988   \n",
       "1047  https://www.discogs.com/Elton-John-A-Single-Ma...         1978   \n",
       "1048  https://www.discogs.com/David-Bowie-Never-Let-...         1987   \n",
       "1049  https://www.discogs.com/Daryl-Hall-John-Oates-...         1982   \n",
       "\n",
       "                          label release_country  \n",
       "1045                 Parlophone              UK  \n",
       "1046             London Records              UK  \n",
       "1047  The Rocket Record Company              UK  \n",
       "1048                EMI America              UK  \n",
       "1049                        RCA              US  "
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "top_all_collected.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1050 entries, 0 to 1049\n",
      "Data columns (total 7 columns):\n",
      " #   Column           Non-Null Count  Dtype \n",
      "---  ------           --------------  ----- \n",
      " 0   artists          1050 non-null   object\n",
      " 1   albums           1050 non-null   object\n",
      " 2   artist_links     1050 non-null   object\n",
      " 3   album_links      1050 non-null   object\n",
      " 4   release_year     1047 non-null   object\n",
      " 5   label            1050 non-null   object\n",
      " 6   release_country  1049 non-null   object\n",
      "dtypes: object(7)\n",
      "memory usage: 57.5+ KB\n"
     ]
    }
   ],
   "source": [
    "top_all_collected.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('top_1050_collected_records.pickle', 'wb') as save_file:\n",
    "    pickle.dump(top_all_collected, save_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scraping the main album pages for song info, and links to the first album version"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since albums come in many versions (or pressings), the price varies a lot based on the pressing number. Original pressings are worth the most usually. Since most popular albums were pressed around 100 times or more, I'd have to access that many pages to get the price for each pressing. I decided to focus on predicting the price of the first pressing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "first_release_links = []\n",
    "number_of_songs= []\n",
    "average_song_length= []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The loop below scrapes the main album pages, gets the link to the first realse of that album, and also counts the number of songs, and calculates the average run time of a song on that album. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "for url in top_all_collected['album_links']:\n",
    "    #from album main page\n",
    "    time.sleep(10+2*random.random())\n",
    "    response = requests.get(url, headers = user_agent)\n",
    "    page = response.text\n",
    "    soup = BeautifulSoup(page, \"lxml\")\n",
    "    \n",
    "    #link to album 1st release page\n",
    "    try:\n",
    "        first_release_links.append('https://www.discogs.com' + soup.find('table', id='versions').find('td', class_='title').find_all('a')[0].get('href'))\n",
    "    except:\n",
    "        first_release_links.append(np.nan)\n",
    "    \n",
    "    #number of songs\n",
    "    s=0\n",
    "    try:\n",
    "        for element in soup.find('table', class_=\"playlist\").find_all('tr', class_=\"tracklist_track track\"):\n",
    "            for song in element.find('span', class_=\"tracklist_track_title\"):\n",
    "                s += 1\n",
    "        number_of_songs.append(s)\n",
    "    except:\n",
    "        number_of_songs.append(np.nan)\n",
    "    \n",
    "    #average song length\n",
    "    form='%M:%S'\n",
    "    song_lengths=[]\n",
    "    try:\n",
    "        for element in soup.find('table', class_=\"playlist\").find_all('tr', class_=\"tracklist_track track\"):\n",
    "            t = element.find('td', class_=\"tracklist_track_duration\").find('span').text\n",
    "            t = datetime.strptime(t,form)\n",
    "            song_lengths.append(t)\n",
    "        avg_time=datetime.strftime(datetime.fromtimestamp(sum(map(datetime.timestamp,song_lengths))/len(song_lengths)),\"%M:%S\")\n",
    "        average_song_length.append(avg_time)\n",
    "    except:\n",
    "        average_song_length.append(np.nan)\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "album_main_pages = pd.DataFrame({'first_release_links':first_release_links,'number_of_songs':number_of_songs,'average_song_length':average_song_length})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I noticed that the number of songs was one lower than expected for each album. I am not sure why based on the code above, but will look into this. It's possible average song length is also missing one song. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "album_main_pages['number_of_songs']=album_main_pages['number_of_songs']+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1050 entries, 0 to 1049\n",
      "Data columns (total 3 columns):\n",
      " #   Column               Non-Null Count  Dtype \n",
      "---  ------               --------------  ----- \n",
      " 0   first_release_links  1050 non-null   object\n",
      " 1   number_of_songs      1050 non-null   int64 \n",
      " 2   average_song_length  831 non-null    object\n",
      "dtypes: int64(1), object(2)\n",
      "memory usage: 24.7+ KB\n"
     ]
    }
   ],
   "source": [
    "album_main_pages.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('album_main_pages_2.pickle', 'wb') as save_file:\n",
    "    pickle.dump(album_main_pages, save_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scraping the artist pages for data on number of albums, and years of albums"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This uses the artist links from the top_1050_collected_records table. I thought that sorting the artist's albums in reverse to get the last year of a release would be a good idea, but a lot of the artists have compilation albums released years after the band broke up. Surprisingly, the year the band broke up is not readily available."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_artist_albums = []\n",
    "artist_first_years = []\n",
    "artist_last_years = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "for url in top_1050_collected_records['artist_links']:\n",
    "    #get info from artist pages\n",
    "    response = requests.get(url, headers = user_agent)\n",
    "    time.sleep(5+1*random.random())\n",
    "    page = response.text\n",
    "    soup = BeautifulSoup(page, \"lxml\")\n",
    "    \n",
    "    #total albums\n",
    "    try:\n",
    "        total_artist_albums.append(soup.find(id=\"discography_wrapper\").find_all('span', class_='facet_count')[1].text)\n",
    "    except:\n",
    "        total_artist_albums.append(np.nan)\n",
    "        \n",
    "    #first album year\n",
    "    try:\n",
    "        artist_first_years.append(soup.find('table', id=\"artist\").find_all('td', class_=\"year has_header\")[0].text)\n",
    "    except:\n",
    "        artist_first_years.append(np.nan)\n",
    "        \n",
    "    #last album year\n",
    "    try:\n",
    "        url=url + \"?sort=year%2Cdesc&limit=25\"\n",
    "        time.sleep(5+1*random.random())\n",
    "        response = requests.get(url, headers = user_agent)\n",
    "        page = response.text\n",
    "        soup = BeautifulSoup(page, \"lxml\")\n",
    "        artist_last_years.append(soup.find('table', id=\"artist\").find_all('td', class_=\"year has_header\")[0].text)\n",
    "    except:\n",
    "        artist_last_years.append(np.nan)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "artist_data = pd.DataFrame({'total_artist_albums':total_artist_albums,'artist_last_years':artist_last_years,'artist_first_years':artist_first_years})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('artist_data.pickle', 'wb') as write_file:\n",
    "    pickle.dump(artist_data, write_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scraping the first release page for each album for many stats and the target variable of median price"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "users_have = []\n",
    "users_want = []\n",
    "user_rating = []\n",
    "lowest_price = []\n",
    "median_price = []\n",
    "highest_price = []\n",
    "last_sold = []\n",
    "number_for_sale = []\n",
    "styles = []\n",
    "genres = []\n",
    "versions = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "for url in album_main_pages['first_release_links'][46:]:\n",
    "    #get info from the first release of an album page\n",
    "    response = requests.get(url, headers = user_agent)\n",
    "    time.sleep(5+1*random.random())\n",
    "    page = response.text\n",
    "    soup = BeautifulSoup(page, \"lxml\")\n",
    "    \n",
    "    #getting info for the record stats, including price\n",
    "    try:\n",
    "        stats =[element for element in soup.find(id = \"statistics\").find_all('li')]\n",
    "    except:\n",
    "        users_have.append(np.nan)\n",
    "        users_want.append(np.nan)\n",
    "        user_rating.append(np.nan)\n",
    "        lowest_price.append(np.nan)\n",
    "        median_price.append(np.nan)\n",
    "        highest_price.append(np.nan)\n",
    "        last_sold.append(np.nan)\n",
    "        #the below stats data try and except clauses should have been \n",
    "        #deleted. This resulted in an extra row after each album with no stats\n",
    "    \n",
    "    #Below this should have been deleted\n",
    "    try:\n",
    "        users_have.append(re.search(':\\n([0-9]*)',stats[0].text).group(1))\n",
    "    except:\n",
    "        users_have.append(np.nan)\n",
    "    try:\n",
    "        users_want.append(re.search(':\\n([0-9]*)',stats[1].text).group(1))\n",
    "    except:\n",
    "        users_want.append(np.nan)\n",
    "    try:\n",
    "        user_rating.append(re.search(':\\n([0-9].[0-9]{1,2})', stats[2].text).group(1))\n",
    "    except:\n",
    "        user_rating.append(np.nan)\n",
    "    try:\n",
    "        lowest_price.append(re.search('\\$([0-9,]*.[0-9]{2})',stats[5].text).group(1))\n",
    "    except:\n",
    "        lowest_price.append(np.nan)\n",
    "    try:\n",
    "        median_price.append(re.search('\\$([0-9,]*.[0-9]{2})',stats[6].text).group(1))\n",
    "    except:\n",
    "        median_price.append(np.nan)\n",
    "    try:\n",
    "        highest_price.append(re.search('\\$([0-9,]*.[0-9]{2})',stats[7].text).group(1))\n",
    "    except:\n",
    "        highest_price.append(np.nan)\n",
    "    try:\n",
    "        last_sold.append(re.search(':\\n([0-9]{2} [A-z]{3} [0-9]{2})',stats[4].text).group(1))\n",
    "    except:\n",
    "        last_sold.append(np.nan)\n",
    "    #Above this should have been deleted \n",
    "        \n",
    "        \n",
    "    #the number of versions\n",
    "    try:\n",
    "        versions.append(re.search('of ([0-9]*)\\)',soup.find('h3',{'data-for':'.m_versions'}).text).group(1))\n",
    "    except:\n",
    "        versions.append(np.nan)\n",
    "        \n",
    "    #getting for sale info from marketplace section\n",
    "    try:\n",
    "        element = soup.find(class_ = \"marketplace_for_sale_count\").find('strong').text\n",
    "        number_for_sale.append(re.search('([0-9,]*)',element).group())\n",
    "    except:\n",
    "        number_for_sale.append(np.nan)\n",
    "        \n",
    "    #find genre and styles (may be multiple, so a list)\n",
    "    try:\n",
    "        all_styles=[]\n",
    "        for link in soup.find(class_=\"profile\").find_all(\"a\"):\n",
    "            if 'style' in link.get('href'):\n",
    "                all_styles.append(link.text)\n",
    "        styles.append(all_styles)\n",
    "    except:\n",
    "        styles.append(np.nan)\n",
    "        \n",
    "    try:\n",
    "        all_genres=[]\n",
    "        for link in soup.find(class_=\"profile\").find_all(\"a\"):\n",
    "            if 'genre' in link.get('href'):\n",
    "                all_genres.append(link.text)\n",
    "        genres.append(all_genres)\n",
    "    except:\n",
    "        genres.append(np.nan)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "arrays must all be same length",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-58-10e077d7aa82>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mfirst_version_page_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m'users_have'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0musers_have\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'users_want'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0musers_want\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'user_rating'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0muser_rating\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'lowest_price'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlowest_price\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'median_price'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mmedian_price\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'highest_price'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mhighest_price\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'last_sold'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlast_sold\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'number_for_sale'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mnumber_for_sale\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'styles'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mstyles\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'genres'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mgenres\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'versions'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mversions\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/opt/anaconda3/lib/python3.7/site-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, data, index, columns, dtype, copy)\u001b[0m\n\u001b[1;32m    433\u001b[0m             )\n\u001b[1;32m    434\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 435\u001b[0;31m             \u001b[0mmgr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minit_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolumns\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    436\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mma\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mMaskedArray\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    437\u001b[0m             \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mma\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmrecords\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mmrecords\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.7/site-packages/pandas/core/internals/construction.py\u001b[0m in \u001b[0;36minit_dict\u001b[0;34m(data, index, columns, dtype)\u001b[0m\n\u001b[1;32m    252\u001b[0m             \u001b[0marr\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mis_datetime64tz_dtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marr\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0marr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0marr\u001b[0m \u001b[0;32min\u001b[0m \u001b[0marrays\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    253\u001b[0m         ]\n\u001b[0;32m--> 254\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0marrays_to_mgr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marrays\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata_names\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolumns\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    255\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    256\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.7/site-packages/pandas/core/internals/construction.py\u001b[0m in \u001b[0;36marrays_to_mgr\u001b[0;34m(arrays, arr_names, index, columns, dtype)\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[0;31m# figure out the index, if necessary\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mindex\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 64\u001b[0;31m         \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mextract_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marrays\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     65\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m         \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mensure_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.7/site-packages/pandas/core/internals/construction.py\u001b[0m in \u001b[0;36mextract_index\u001b[0;34m(data)\u001b[0m\n\u001b[1;32m    363\u001b[0m             \u001b[0mlengths\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mraw_lengths\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    364\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlengths\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 365\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"arrays must all be same length\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    366\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    367\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mhave_dicts\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: arrays must all be same length"
     ]
    }
   ],
   "source": [
    "first_version_page_df = pd.DataFrame({'users_have':users_have,'users_want':users_want,'user_rating':user_rating,'lowest_price':lowest_price,'median_price':median_price,'highest_price':highest_price,'last_sold':last_sold,'number_for_sale':number_for_sale,'styles':styles,'genres':genres,'versions':versions})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1068"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(users_have)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1068"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(users_want)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1068"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(user_rating)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1068"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(lowest_price)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1068"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(highest_price)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1068"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(median_price)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1050"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(styles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1050"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(genres)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1050"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(number_for_sale)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1050"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(versions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The lists that were the expected length were put into df_B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_B = pd.DataFrame({'styles':styles,'genres':genres,'number_for_sale':number_for_sale, 'versions':versions})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1050 entries, 0 to 1049\n",
      "Data columns (total 4 columns):\n",
      " #   Column           Non-Null Count  Dtype \n",
      "---  ------           --------------  ----- \n",
      " 0   styles           1049 non-null   object\n",
      " 1   genres           1049 non-null   object\n",
      " 2   number_for_sale  1034 non-null   object\n",
      " 3   versions         1033 non-null   object\n",
      "dtypes: object(4)\n",
      "memory usage: 32.9+ KB\n"
     ]
    }
   ],
   "source": [
    "df_B.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "df_A wil contain the stats lists that were too long for more investigation. I found that there are duplicate rows caused by an additional row being added after each NaN row for stats. Stats was sometimes not found, if the link from the original top_1050_records dataframe to the album main page, actually went to the specific album version page. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_A = pd.DataFrame({'users_have':users_have,'users_want':users_want,'user_rating':user_rating,'lowest_price':lowest_price,'median_price':median_price,'highest_price':highest_price,'last_sold':last_sold})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1068 entries, 0 to 1067\n",
      "Data columns (total 7 columns):\n",
      " #   Column         Non-Null Count  Dtype \n",
      "---  ------         --------------  ----- \n",
      " 0   users_have     1050 non-null   object\n",
      " 1   users_want     1050 non-null   object\n",
      " 2   user_rating    1048 non-null   object\n",
      " 3   lowest_price   1045 non-null   object\n",
      " 4   median_price   1045 non-null   object\n",
      " 5   highest_price  1045 non-null   object\n",
      " 6   last_sold      1045 non-null   object\n",
      "dtypes: object(7)\n",
      "memory usage: 58.5+ KB\n"
     ]
    }
   ],
   "source": [
    "df_A.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>users_have</th>\n",
       "      <th>users_want</th>\n",
       "      <th>user_rating</th>\n",
       "      <th>lowest_price</th>\n",
       "      <th>median_price</th>\n",
       "      <th>highest_price</th>\n",
       "      <th>last_sold</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>207</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>333</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>343</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>357</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>426</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>561</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>571</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>600</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>615</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>718</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>745</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>784</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>836</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>943</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1007</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1059</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     users_have users_want user_rating lowest_price median_price  \\\n",
       "46          NaN        NaN         NaN          NaN          NaN   \n",
       "97          NaN        NaN         NaN          NaN          NaN   \n",
       "207         NaN        NaN         NaN          NaN          NaN   \n",
       "333         NaN        NaN         NaN          NaN          NaN   \n",
       "343         NaN        NaN         NaN          NaN          NaN   \n",
       "357         NaN        NaN         NaN          NaN          NaN   \n",
       "426         NaN        NaN         NaN          NaN          NaN   \n",
       "561         NaN        NaN         NaN          NaN          NaN   \n",
       "571         NaN        NaN         NaN          NaN          NaN   \n",
       "600         NaN        NaN         NaN          NaN          NaN   \n",
       "615         NaN        NaN         NaN          NaN          NaN   \n",
       "718         NaN        NaN         NaN          NaN          NaN   \n",
       "745         NaN        NaN         NaN          NaN          NaN   \n",
       "784         NaN        NaN         NaN          NaN          NaN   \n",
       "836         NaN        NaN         NaN          NaN          NaN   \n",
       "943         NaN        NaN         NaN          NaN          NaN   \n",
       "1007        NaN        NaN         NaN          NaN          NaN   \n",
       "1059        NaN        NaN         NaN          NaN          NaN   \n",
       "\n",
       "     highest_price last_sold  \n",
       "46             NaN       NaN  \n",
       "97             NaN       NaN  \n",
       "207            NaN       NaN  \n",
       "333            NaN       NaN  \n",
       "343            NaN       NaN  \n",
       "357            NaN       NaN  \n",
       "426            NaN       NaN  \n",
       "561            NaN       NaN  \n",
       "571            NaN       NaN  \n",
       "600            NaN       NaN  \n",
       "615            NaN       NaN  \n",
       "718            NaN       NaN  \n",
       "745            NaN       NaN  \n",
       "784            NaN       NaN  \n",
       "836            NaN       NaN  \n",
       "943            NaN       NaN  \n",
       "1007           NaN       NaN  \n",
       "1059           NaN       NaN  "
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_A[df_A['users_have'].isnull()==True]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Noticing now that the issue was the extra line appended after each NaN line. This extra line is the same as the line before the NaN line."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>users_have</th>\n",
       "      <th>users_want</th>\n",
       "      <th>user_rating</th>\n",
       "      <th>lowest_price</th>\n",
       "      <th>median_price</th>\n",
       "      <th>highest_price</th>\n",
       "      <th>last_sold</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>18941</td>\n",
       "      <td>2177</td>\n",
       "      <td>4.24</td>\n",
       "      <td>0.99</td>\n",
       "      <td>4.50</td>\n",
       "      <td>20.00</td>\n",
       "      <td>19 Sep 20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>1799</td>\n",
       "      <td>3478</td>\n",
       "      <td>4.55</td>\n",
       "      <td>32.47</td>\n",
       "      <td>97.73</td>\n",
       "      <td>168.83</td>\n",
       "      <td>04 Sep 20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>8877</td>\n",
       "      <td>2590</td>\n",
       "      <td>4.49</td>\n",
       "      <td>2.53</td>\n",
       "      <td>12.99</td>\n",
       "      <td>29.99</td>\n",
       "      <td>31 Aug 20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>6987</td>\n",
       "      <td>2854</td>\n",
       "      <td>4.42</td>\n",
       "      <td>15.58</td>\n",
       "      <td>25.96</td>\n",
       "      <td>64.94</td>\n",
       "      <td>03 Sep 20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>8062</td>\n",
       "      <td>2159</td>\n",
       "      <td>4.38</td>\n",
       "      <td>6.00</td>\n",
       "      <td>14.27</td>\n",
       "      <td>29.00</td>\n",
       "      <td>09 Sep 20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>1781</td>\n",
       "      <td>1941</td>\n",
       "      <td>4.26</td>\n",
       "      <td>32.93</td>\n",
       "      <td>104.89</td>\n",
       "      <td>199.00</td>\n",
       "      <td>24 Aug 20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>1781</td>\n",
       "      <td>1941</td>\n",
       "      <td>4.26</td>\n",
       "      <td>32.93</td>\n",
       "      <td>104.89</td>\n",
       "      <td>199.00</td>\n",
       "      <td>24 Aug 20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>9224</td>\n",
       "      <td>2420</td>\n",
       "      <td>4.45</td>\n",
       "      <td>9.74</td>\n",
       "      <td>12.99</td>\n",
       "      <td>42.86</td>\n",
       "      <td>01 Sep 20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>3104</td>\n",
       "      <td>2656</td>\n",
       "      <td>4.41</td>\n",
       "      <td>25.96</td>\n",
       "      <td>199.99</td>\n",
       "      <td>294.12</td>\n",
       "      <td>07 Aug 20</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   users_have users_want user_rating lowest_price median_price highest_price  \\\n",
       "40      18941       2177        4.24         0.99         4.50         20.00   \n",
       "41       1799       3478        4.55        32.47        97.73        168.83   \n",
       "42       8877       2590        4.49         2.53        12.99         29.99   \n",
       "43       6987       2854        4.42        15.58        25.96         64.94   \n",
       "44       8062       2159        4.38         6.00        14.27         29.00   \n",
       "45       1781       1941        4.26        32.93       104.89        199.00   \n",
       "46        NaN        NaN         NaN          NaN          NaN           NaN   \n",
       "47       1781       1941        4.26        32.93       104.89        199.00   \n",
       "48       9224       2420        4.45         9.74        12.99         42.86   \n",
       "49       3104       2656        4.41        25.96       199.99        294.12   \n",
       "\n",
       "    last_sold  \n",
       "40  19 Sep 20  \n",
       "41  04 Sep 20  \n",
       "42  31 Aug 20  \n",
       "43  03 Sep 20  \n",
       "44  09 Sep 20  \n",
       "45  24 Aug 20  \n",
       "46        NaN  \n",
       "47  24 Aug 20  \n",
       "48  01 Sep 20  \n",
       "49  07 Aug 20  "
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_A.iloc[40:50]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>users_have</th>\n",
       "      <th>users_want</th>\n",
       "      <th>user_rating</th>\n",
       "      <th>lowest_price</th>\n",
       "      <th>median_price</th>\n",
       "      <th>highest_price</th>\n",
       "      <th>last_sold</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>200</th>\n",
       "      <td>3991</td>\n",
       "      <td>1121</td>\n",
       "      <td>4.13</td>\n",
       "      <td>12.97</td>\n",
       "      <td>25.32</td>\n",
       "      <td>78.71</td>\n",
       "      <td>05 Sep 20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>201</th>\n",
       "      <td>4199</td>\n",
       "      <td>1392</td>\n",
       "      <td>4.08</td>\n",
       "      <td>6.49</td>\n",
       "      <td>12.99</td>\n",
       "      <td>29.86</td>\n",
       "      <td>06 Sep 20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>202</th>\n",
       "      <td>686</td>\n",
       "      <td>663</td>\n",
       "      <td>4.48</td>\n",
       "      <td>20.00</td>\n",
       "      <td>37.50</td>\n",
       "      <td>99.99</td>\n",
       "      <td>26 Jul 20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>203</th>\n",
       "      <td>11533</td>\n",
       "      <td>2731</td>\n",
       "      <td>4.5</td>\n",
       "      <td>6.99</td>\n",
       "      <td>18.88</td>\n",
       "      <td>99.99</td>\n",
       "      <td>07 Sep 20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>204</th>\n",
       "      <td>1224</td>\n",
       "      <td>3533</td>\n",
       "      <td>4.32</td>\n",
       "      <td>38.90</td>\n",
       "      <td>338.80</td>\n",
       "      <td>924.46</td>\n",
       "      <td>29 Jun 20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>205</th>\n",
       "      <td>6271</td>\n",
       "      <td>6390</td>\n",
       "      <td>4.67</td>\n",
       "      <td>30.00</td>\n",
       "      <td>75.00</td>\n",
       "      <td>3,000.00</td>\n",
       "      <td>06 Aug 20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>206</th>\n",
       "      <td>7199</td>\n",
       "      <td>276</td>\n",
       "      <td>3.97</td>\n",
       "      <td>2.94</td>\n",
       "      <td>5.33</td>\n",
       "      <td>9.08</td>\n",
       "      <td>22 Aug 20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>207</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>208</th>\n",
       "      <td>7199</td>\n",
       "      <td>276</td>\n",
       "      <td>3.97</td>\n",
       "      <td>2.94</td>\n",
       "      <td>5.33</td>\n",
       "      <td>9.08</td>\n",
       "      <td>22 Aug 20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>209</th>\n",
       "      <td>1696</td>\n",
       "      <td>1147</td>\n",
       "      <td>4.1</td>\n",
       "      <td>25.88</td>\n",
       "      <td>34.56</td>\n",
       "      <td>63.64</td>\n",
       "      <td>17 Aug 20</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    users_have users_want user_rating lowest_price median_price highest_price  \\\n",
       "200       3991       1121        4.13        12.97        25.32         78.71   \n",
       "201       4199       1392        4.08         6.49        12.99         29.86   \n",
       "202        686        663        4.48        20.00        37.50         99.99   \n",
       "203      11533       2731         4.5         6.99        18.88         99.99   \n",
       "204       1224       3533        4.32        38.90       338.80        924.46   \n",
       "205       6271       6390        4.67        30.00        75.00      3,000.00   \n",
       "206       7199        276        3.97         2.94         5.33          9.08   \n",
       "207        NaN        NaN         NaN          NaN          NaN           NaN   \n",
       "208       7199        276        3.97         2.94         5.33          9.08   \n",
       "209       1696       1147         4.1        25.88        34.56         63.64   \n",
       "\n",
       "     last_sold  \n",
       "200  05 Sep 20  \n",
       "201  06 Sep 20  \n",
       "202  26 Jul 20  \n",
       "203  07 Sep 20  \n",
       "204  29 Jun 20  \n",
       "205  06 Aug 20  \n",
       "206  22 Aug 20  \n",
       "207        NaN  \n",
       "208  22 Aug 20  \n",
       "209  17 Aug 20  "
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_A.iloc[200:210]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Removing the duplicate lines from df_A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "duplicates_after_NAs = list(map(lambda x: x+1, list(df_A[df_A['users_have'].isnull()==True].index)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "18"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(duplicates_after_NAs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_A = df_A.drop(duplicates_after_NAs, axis = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1050, 7)"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_A.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now combing the first edition album df_A and df_B into one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "first_edition_album_info = pd.concat([df_A, df_B],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1067, 11)"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "first_edition_album_info.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_A.reset_index(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "first_edition_album_info = pd.concat([df_A, df_B],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1050, 12)"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "first_edition_album_info.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('first_edition_album_info.pickle', 'wb') as write_file:\n",
    "    pickle.dump(first_edition_album_info, write_file)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1050 entries, 0 to 1049\n",
      "Data columns (total 12 columns):\n",
      " #   Column           Non-Null Count  Dtype \n",
      "---  ------           --------------  ----- \n",
      " 0   index            1050 non-null   int64 \n",
      " 1   users_have       1032 non-null   object\n",
      " 2   users_want       1032 non-null   object\n",
      " 3   user_rating      1030 non-null   object\n",
      " 4   lowest_price     1027 non-null   object\n",
      " 5   median_price     1027 non-null   object\n",
      " 6   highest_price    1027 non-null   object\n",
      " 7   last_sold        1027 non-null   object\n",
      " 8   styles           1049 non-null   object\n",
      " 9   genres           1049 non-null   object\n",
      " 10  number_for_sale  1034 non-null   object\n",
      " 11  versions         1033 non-null   object\n",
      "dtypes: int64(1), object(11)\n",
      "memory usage: 98.6+ KB\n"
     ]
    }
   ],
   "source": [
    "first_edition_album_info.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('album_main_pages_2.pickle', 'wb') as write_file:\n",
    "    pickle.dump(album_main_pages, write_file)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
